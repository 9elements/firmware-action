// SPDX-License-Identifier: MIT

// Package recipes yay!
package recipes

import (
	"context"
	"errors"
	"fmt"
	"io"
	"log/slog"
	"os"
	"path/filepath"
	"slices"
	"sync"

	"dagger.io/dagger"
	"github.com/9elements/firmware-action/cmd/firmware-action/container"
	"github.com/9elements/firmware-action/cmd/firmware-action/environment"
	"github.com/9elements/firmware-action/cmd/firmware-action/filesystem"
	"github.com/heimdalr/dag"
)

// Errors for recipes
var (
	ErrBuildFailed               = errors.New("build failed")
	ErrBuildUpToDate             = errors.New("build is up-to-date")
	ErrDependencyTreeUndefDep    = errors.New("module has invalid dependency")
	ErrDependencyTreeUnderTarget = errors.New("target not found in dependency tree")
	ErrDependencyOutputMissing   = errors.New("output of one or more dependencies is missing")
	ErrFailedValidation          = errors.New("config failed validation")
	ErrTargetInvalid             = errors.New("unsupported target")
	ErrTargetMissing             = errors.New("no target specified")
)

var (
	// ContainerWorkDir specifies directory in container used as work directory
	ContainerWorkDir = "/workdir"
	// StatusDir is directory for temporary files generated by firmware-action to aid change detection
	StatusDir = ".firmware-action"
	// TimestampsDir specifies directory for timestamps to detect changes in sources
	TimestampsDir = filepath.Join(StatusDir, "timestamps")
	// CompiledConfigsDir specifies directory for successfully compiled module configurations to detect changes in
	//   configuration
	CompiledConfigsDir = filepath.Join(StatusDir, "configs")
	// GitRepoHashDir specifies directory for git hashes to detect changes in sources
	GitRepoHashDir = filepath.Join(StatusDir, "git-hashes")
	// ArtifactDir specifies directory where to store a copy of artifacts for caching in CI
	ArtifactDir = filepath.Join(StatusDir, "artifacts")
)

func forestAddVertex(forest *dag.DAG, key string, value FirmwareModule, dependencies [][]string) ([][]string, error) {
	err := forest.AddVertexByID(key, key)
	if err != nil {
		return nil, err
	}
	for _, dep := range value.GetDepends() {
		dependencies = append(dependencies, []string{key, dep})
	}
	return dependencies, nil
}

// BuildResults contains target name and result of its build
type BuildResults struct {
	Name        string
	BuildResult error
}

// Build recipes, possibly recursively
func Build(
	ctx context.Context,
	target string,
	recursive bool,
	pruneDocker bool,
	config *Config,
	executor func(context.Context, string, *Config) error,
) ([]BuildResults, error) {
	dependencyForest := dag.NewDAG()
	dependencies := [][]string{}
	var err error

	// Create the forest (forest = multiple independent trees)
	//   Add all items as vertexes into the tree
	for key, value := range config.AllModules() {
		dependencies, err = forestAddVertex(dependencyForest, key, value, dependencies)
		if err != nil {
			return nil, err
		}
	}

	// Add edges
	//   Edges must be added after all vertexes were are added
	for _, dep := range dependencies {
		err = dependencyForest.AddEdge(dep[0], dep[1])
		if err != nil {
			return nil, fmt.Errorf("%w: %w", ErrDependencyTreeUndefDep, err)
		}
	}

	// Check target is in Forest
	_, err = dependencyForest.GetVertex(target)
	if err != nil {
		return nil, fmt.Errorf("%w: %w", ErrDependencyTreeUnderTarget, err)
	}

	// Create a queue in correct order (starting with leaves)
	queue := []string{}
	queueMutex := &sync.Mutex{} // Mutex to ensure concurrent access to queue is safe in the callback
	flowCallback := func(d *dag.DAG, id string, _ []dag.FlowResult) (interface{}, error) {
		v, err := d.GetVertex(id)
		if err != nil {
			return nil, err
		}
		queueMutex.Lock()
		queue = append(queue, v.(string))
		queueMutex.Unlock()
		return nil, nil
	}

	// Create a subgraph with target as the only root
	// Having multiple roots will result in miscalculation inside DescendantsFlow channel size calculation
	pruned, rootID, err := dependencyForest.GetDescendantsGraph(target)
	if err != nil {
		return nil, err
	}

	_, err = pruned.DescendantsFlow(rootID, nil, flowCallback)
	if err != nil {
		return nil, err
	}
	slices.Reverse(queue)

	// Build each item in queue (if recursive)
	slog.Info(fmt.Sprintf("Building queue: %v", queue))
	builds := []BuildResults{}
	if recursive {
		slog.Info(fmt.Sprintf("Building '%s' recursively", target))
		for _, item := range queue {
			slog.Info(fmt.Sprintf("Building: %s", item))

			err = executor(ctx, item, config)
			builds = append(builds, BuildResults{item, err})

			if err != nil && !errors.Is(err, ErrBuildUpToDate) {
				break
			}

			// Prune the Dagger Engine to free disk space
			// It is fine to do here because the `executor` function connects and disconnects
			//   to Dagger Engine (it is self-contained)
			if pruneDocker {
				err = container.CleanupAfterContainer(ctx)
				if err != nil {
					break
				}
			}
		}
	} else {
		// else build only the target
		slog.Info(fmt.Sprintf("Building '%s' NOT recursively", target))

		err = executor(ctx, target, config)
		builds = append(builds, BuildResults{target, err})
	}

	// Check results
	err = nil
	for _, item := range builds {
		if item.BuildResult != nil && !errors.Is(item.BuildResult, ErrBuildUpToDate) {
			err = item.BuildResult
		}
	}

	return builds, err
}

// IsDirEmpty returns whether given directory is empty or not
func IsDirEmpty(path string) (bool, error) {
	// Source: https://stackoverflow.com/questions/30697324/how-to-check-if-directory-on-path-is-empty
	f, err := os.Open(path)
	if err != nil {
		return false, err
	}
	defer f.Close()

	// File.Readdirnames() take a parameter which is used to limit the number of returned values
	// It is enough to query only 1 child
	// File.Readdirnames() is faster than File.Readdir()
	_, err = f.Readdirnames(1)
	if err == io.EOF {
		return true, nil
	}
	return false, err // Either not empty or error, suits both cases
}

// Execute a build step
// func Execute(ctx context.Context, target string, config *Config, bulldozeMode bool) error {
func Execute(ctx context.Context, target string, config *Config) error {
	// Prep directories
	for _, dir := range []string{TimestampsDir, CompiledConfigsDir, GitRepoHashDir} {
		err := os.MkdirAll(dir, os.ModePerm)
		if err != nil {
			return err
		}
	}

	// Find requested target
	modules := config.AllModules()
	if _, ok := modules[target]; ok {
		// Check for any change in source files
		detectedChanges := AllChanges{
			TimeStamp: ChangeTimeStamp{
				Change: Change{
					ResultFile: filepath.Join(TimestampsDir, filesystem.Filenamify(target, "txt")),
				},
				Sources: modules[target].GetSources(),
			},
			Configuration: ChangeConfig{
				Change: Change{
					ResultFile: filepath.Join(CompiledConfigsDir, filesystem.Filenamify(target, "json")),
				},
				Config: config,
			},
			GitHash: ChangeGitHash{
				Change: Change{
					ResultFile: filepath.Join(GitRepoHashDir, filesystem.Filenamify(target, "txt")),
				},
				RepoPath: modules[target].GetRepoPath(),
			},
		}

		// Check if output directory already exist
		// We want to skip build if the output directory exists and is not empty
		// If it is empty, then just continue with the building
		// If changes in sources were detected, re-build
		_, errExists := os.Stat(modules[target].GetOutputDir())
		empty, _ := IsDirEmpty(modules[target].GetOutputDir())
		if errExists == nil && !empty {
			if detectedChanges.DetectChanges(target) {
				// If any of the sources changed, we need to rebuild
				slog.Debug(
					fmt.Sprintf("Deleting '%s'", modules[target].GetOutputDir()),
				)
				os.RemoveAll(modules[target].GetOutputDir())
			} else {
				// Is already up-to-date
				slog.Warn(fmt.Sprintf("Target '%s' is up-to-date, skipping build", target))

				// It is possible that the timestamp or old configuration files are missing
				//   for example user deleted them, CI does not cache them, ...
				//   but at the same time the output directory exists with all the artifacts
				// If the 'override' is false, and if these files already exist, they will not
				//   be overridden
				// We want these files to reflect last successful build, not last check
				detectedChanges.SaveCheckpoint(target, false)
				return ErrBuildUpToDate
			}
		}

		// Check if all outputs of required modules exist
		for _, prerequisite := range modules[target].GetDepends() {
			outputDir := modules[prerequisite].GetOutputDir()
			paths := modules[prerequisite].GetContainerOutputDirs()
			paths = append(paths, modules[prerequisite].GetContainerOutputFiles()...)

			for _, path := range paths {
				finalPath := filepath.Join(outputDir, filepath.Base(path))
				slog.Info(finalPath)
				if _, err := os.Stat(finalPath); os.IsNotExist(err) {
					slog.Error(
						"Missing output files and/or directories from one or more required module(s) defined in 'Depends'",
						slog.String("suggestion", "build needed modules or use '--recursive' build"),
						slog.Any("error", errors.Join(err, ErrDependencyOutputMissing)),
					)
					return ErrDependencyOutputMissing
				}
			}
		}

		// Setup dagger client
		environment.LogGroupStart("connect to dagger engine")
		//   this will make around 400 lines of irrelevant-to-the-user log collapsible
		client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stdout))
		if err != nil {
			return err
		}
		defer client.Close()
		environment.LogGroupStop("connect to dagger engine")

		// Build the module
		err = modules[target].buildFirmware(ctx, client)
		if err == nil {
			// On successful build, save checkpoint data for next change detection
			detectedChanges.SaveCheckpoint(target, true)
			if environment.DetectGithub() {
				// If in GitHub, copy output directory into 'StatusDir', so that it can be cached
				//   and also automatically uploaded as artifact
				dirToCache := modules[target].GetOutputDir()
				// .GetOutputDir() returns verbatim what is defined in the configuration file, no
				//   pre-processing or any alterations
				//   (must be relative path)
				pathToCache := filepath.Join(ArtifactDir, filepath.Base(dirToCache))

				slog.Debug(
					fmt.Sprintf("Copying output files to '%s' for caching and artifact upload", StatusDir),
					slog.String("Source", dirToCache),
					slog.String("Destination", pathToCache),
				)
				if err := filesystem.CopyDir(dirToCache, pathToCache); err != nil {
					return err
				}

				// Store path to the original directory which was cached
				//   (to be used in the GitHub wrapper for unpacking)
				dirToCacheFilename := filepath.Join(ArtifactDir, filepath.Base(dirToCache)+".txt")
				if err := os.WriteFile(dirToCacheFilename, []byte(dirToCache), 0o644); err != nil {
					return err
				}
			}
		}
		return err
	}
	return ErrTargetMissing
}

// NormalizeArchitecture will translate various architecture strings into expected format
func NormalizeArchitecture(arch string) string {
	archMap := map[string]string{
		// x86 32-bit
		"IA-32":  "i386", // Intel
		"IA32":   "i386", // Intel
		"i686":   "i386", // common on Linux
		"386":    "i386", // GOARCH
		"x86":    "i386", // common on Windows
		"x86-32": "i386", // rare
		"x86_32": "i386", // rare
		// x86 64-bit
		"AMD64":  "amd64",
		"x64":    "amd64", // common on Windows
		"x86-64": "amd64",
		"x86_64": "amd64",
	}
	result, ok := archMap[arch]
	if result != "" && ok {
		return result
	}
	// fallback
	return arch
}

// NormalizeArchitectureForLinux will translate various architecture strings into format expected by Linux
func NormalizeArchitectureForLinux(arch string) string {
	normalArch := NormalizeArchitecture(arch)
	archMap := map[string]string{
		// x86 32-bit
		"i386": "x86",
		// x86 64-bit (x86_64 reuses x86)
		"amd64": "x86",
	}
	result, ok := archMap[normalArch]
	if result != "" && ok {
		return result
	}
	// fallback
	return arch
}
